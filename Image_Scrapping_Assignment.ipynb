{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Write a python program to extract the video URL of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_video_data():\n",
    "    url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        video_urls = []\n",
    "        thumbnails_urls = []\n",
    "        titles = []\n",
    "        views = []\n",
    "        posting_times = []\n",
    "\n",
    "        for video in soup.select(\"div#dismissible\"):\n",
    "            video_url = \"https://www.youtube.com\" + video.find(\"a\", href=True)[\"href\"]\n",
    "            thumbnail_url = video.find(\"img\")[\"src\"]\n",
    "            title = video.find(\"a\", {\"id\": \"video-title\"}).text.strip()\n",
    "\n",
    "            # Extracting views\n",
    "            view_text = video.select_one(\"span.style-scope.ytd-grid-video-renderer\").text\n",
    "            view_text = view_text.replace(\" views\", \"\").replace(\",\", \"\")\n",
    "            view_count = int(view_text) if view_text.isdigit() else 0\n",
    "\n",
    "            # Extracting posting time\n",
    "            time_element = video.select_one(\"span.style-scope.ytd-grid-video-renderer > a\")\n",
    "            posting_time = time_element[\"aria-label\"] if time_element else \"\"\n",
    "\n",
    "            video_urls.append(video_url)\n",
    "            thumbnails_urls.append(thumbnail_url)\n",
    "            titles.append(title)\n",
    "            views.append(view_count)\n",
    "            posting_times.append(posting_time)\n",
    "\n",
    "        return video_urls, thumbnails_urls, titles, views, posting_times\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. HTTP Status Code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def scrape_and_save():\n",
    "    # Call the function to extract video data\n",
    "    video_urls, thumbnails_urls, titles, views, posting_times = extract_video_data()\n",
    "\n",
    "    if video_urls:\n",
    "        # Open a CSV file for writing\n",
    "        with open(\"youtube_data.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            # Define fieldnames for the CSV file\n",
    "            fieldnames = [\"Video_URL\", \"Thumbnail_URL\", \"Title\", \"Views\", \"Posting_Time\"]\n",
    "\n",
    "            # Create a CSV writer object\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write data\n",
    "            for i in range(len(video_urls)):\n",
    "                writer.writerow({\n",
    "                    \"Video_URL\": video_urls[i],\n",
    "                    \"Thumbnail_URL\": thumbnails_urls[i],\n",
    "                    \"Title\": titles[i],\n",
    "                    \"Views\": views[i],\n",
    "                    \"Posting_Time\": posting_times[i],\n",
    "                })\n",
    "\n",
    "        print(\"Data scraped and saved to youtube_data.csv\")\n",
    "\n",
    "# Call the function to scrape and save data\n",
    "scrape_and_save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Write a python program to extract the URL of the video thumbnails of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def extract_video_thumbnails():\n",
    "    url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        thumbnails_urls = []\n",
    "\n",
    "        for video in soup.select(\"div#dismissible\"):\n",
    "            thumbnail_url = video.find(\"img\")[\"src\"]\n",
    "            thumbnails_urls.append(thumbnail_url)\n",
    "\n",
    "            if len(thumbnails_urls) == 5:\n",
    "                break  # Stop after collecting thumbnails for the first five videos\n",
    "\n",
    "        return thumbnails_urls\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. HTTP Status Code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def scrape_and_save_thumbnails():\n",
    "    # Call the function to extract video thumbnails\n",
    "    thumbnails_urls = extract_video_thumbnails()\n",
    "\n",
    "    if thumbnails_urls:\n",
    "        # Open a CSV file for writing\n",
    "        with open(\"thumbnails_data.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            # Define fieldnames for the CSV file\n",
    "            fieldnames = [\"Thumbnail_URL\"]\n",
    "\n",
    "            # Create a CSV writer object\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write data\n",
    "            for thumbnail_url in thumbnails_urls:\n",
    "                writer.writerow({\n",
    "                    \"Thumbnail_URL\": thumbnail_url,\n",
    "                })\n",
    "\n",
    "        print(\"Thumbnail URLs scraped and saved to thumbnails_data.csv\")\n",
    "\n",
    "# Call the function to scrape and save thumbnail URLs\n",
    "scrape_and_save_thumbnails()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Write a python program to extract the title of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def extract_video_titles():\n",
    "    url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        titles = []\n",
    "\n",
    "        for video in soup.select(\"div#dismissible\"):\n",
    "            title = video.find(\"a\", {\"id\": \"video-title\"})[\"title\"]\n",
    "            titles.append(title)\n",
    "\n",
    "            if len(titles) == 5:\n",
    "                break  # Stop after collecting titles for the first five videos\n",
    "\n",
    "        return titles\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. HTTP Status Code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def scrape_and_save_titles():\n",
    "    # Call the function to extract video titles\n",
    "    titles = extract_video_titles()\n",
    "\n",
    "    if titles:\n",
    "        # Open a CSV file for writing\n",
    "        with open(\"titles_data.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            # Define fieldnames for the CSV file\n",
    "            fieldnames = [\"Title\"]\n",
    "\n",
    "            # Create a CSV writer object\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write data\n",
    "            for title in titles:\n",
    "                writer.writerow({\n",
    "                    \"Title\": title,\n",
    "                })\n",
    "\n",
    "        print(\"Video titles scraped and saved to titles_data.csv\")\n",
    "\n",
    "# Call the function to scrape and save video titles\n",
    "scrape_and_save_titles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Write a python program to extract the number of views of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def extract_video_views():\n",
    "    url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        views = []\n",
    "\n",
    "        for video in soup.select(\"div#dismissible\"):\n",
    "            view_count = video.find(\"span\", {\"class\": \"style-scope ytd-grid-video-renderer\"}).text.strip()\n",
    "            views.append(view_count)\n",
    "\n",
    "            if len(views) == 5:\n",
    "                break  # Stop after collecting views for the first five videos\n",
    "\n",
    "        return views\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. HTTP Status Code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def scrape_and_save_views():\n",
    "    # Call the function to extract video views\n",
    "    views = extract_video_views()\n",
    "\n",
    "    if views:\n",
    "        # Open a CSV file for writing\n",
    "        with open(\"views_data.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            # Define fieldnames for the CSV file\n",
    "            fieldnames = [\"Views\"]\n",
    "\n",
    "            # Create a CSV writer object\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write data\n",
    "            for view_count in views:\n",
    "                writer.writerow({\n",
    "                    \"Views\": view_count,\n",
    "                })\n",
    "\n",
    "        print(\"Video views scraped and saved to views_data.csv\")\n",
    "\n",
    "# Call the function to scrape and save video views\n",
    "scrape_and_save_views()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Write a python program to extract the time of posting of video for the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def extract_posting_times():\n",
    "    url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        posting_times = []\n",
    "\n",
    "        for video in soup.select(\"div#dismissible\"):\n",
    "            time_posted = video.find(\"span\", {\"class\": \"style-scope ytd-grid-video-renderer\"}).text.strip()\n",
    "            posting_times.append(time_posted)\n",
    "\n",
    "            if len(posting_times) == 5:\n",
    "                break  # Stop after collecting posting times for the first five videos\n",
    "\n",
    "        return posting_times\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. HTTP Status Code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def scrape_and_save_posting_times():\n",
    "    # Call the function to extract posting times\n",
    "    posting_times = extract_posting_times()\n",
    "\n",
    "    if posting_times:\n",
    "        # Open a CSV file for writing\n",
    "        with open(\"posting_times_data.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            # Define fieldnames for the CSV file\n",
    "            fieldnames = [\"Posting_Time\"]\n",
    "\n",
    "            # Create a CSV writer object\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write data\n",
    "            for time_posted in posting_times:\n",
    "                writer.writerow({\n",
    "                    \"Posting_Time\": time_posted,\n",
    "                })\n",
    "\n",
    "        print(\"Posting times scraped and saved to posting_times_data.csv\")\n",
    "\n",
    "# Call the function to scrape and save posting times\n",
    "scrape_and_save_posting_times()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
